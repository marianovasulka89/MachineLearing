{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqxOz/5Q10XV8jEJeZSaXQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AhFSFM6id3PY"},"outputs":[],"source":["\"\"\"\n","Vamos a ir armando perceptrones multicapa, haciendo generalizaciones, para armar la activacion forward\n","\n","y la retropropagacion\n","\n","L capas\n"," \n","Y lista de vectores(matriz) que inicializa vacia con las dimenciones deseadas\n","\n","\n","luego hay que hacer el back propagation, para corregir los pesos\n","\n","\"\"\""]},{"cell_type":"code","source":["import numpy as np\n","\n","#funciones utiles\n","\n","def bias_add(V):\n","\n","  bias = np.ones((len(V) , 1))\n","\n","  return(np.hstack([V,bias]))\n","\n","def bias_sub(V):\n","\n","  return(V[:,:-1])\n","\n","def pesos(var_entrada , var_salida):\n","\n","  return(np.random.normal(0,0.1,(var_entrada+1,var_salida)))"],"metadata":{"id":"nzNNnR18hUxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Para la clase, size es poblada en la activacion, lista de neuronas por capa\n","\"\"\"\n","\n","def activation(Y0, W1):\n","\n","  epocas = 0\n","\n","  while epocas < 10000:\n","\n","    ###FORWARD\n","    Y1[:] = bias_add(np.tanh(np.dot(Y0,W1)))\n","\n","    Y2[:] = (np.tanh(np.dot(Y1 , W2)))\n","\n","  return(np.array[Y1 , Y2])\n","\n","def correction():\n","\n","  epocas = 0\n","\n","  #error = 10000\n","  while(epocas<10000)\n","  #######BACK#######\n","    E2 = z-Y2\n","\n","    D2 = E2*(1-np.square(Y2))#la derivada de Y2\n","\n","    delta_W2 = 0.01*np.dot(Y1.T , D2)\n","  ####ACTUAÃ‘IZO PESOS######\n","    #W2 = W2+delta_W2\n","\n","    E1 = np.dot(D2 , W2.T)\n","\n","    D1 = bias_sub(E1*(1-np.square(Y1)))#saco la dimencion del umbral, el bias, ya que para retropropagar no hace falta, para le prod elemento a elemento\n","\n","    delta_W1 = 0.01*np.dot(Y0.T , D1)\n","\n","    #W1 = W1 + delta_W1\n","\n","  return(np.array[delta_W1 , delta_W2])\n","\n","def adaptation():\n","\n","  epocas = 0\n","\n","  error = 0\n","\n","  while(epoca<1):\n","\n","    W2 = W2+delta_W2\n","\n","    W1 = W1 + delta_W1"],"metadata":{"id":"nri0Y7q4evpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","arranquemos con la func activation\n","\n","tener en cuenta que ne definitiva, vamos a temrinar usando clases para el perceptron multicapa\n","\"\"\"\n","\n","\"\"\"\n","lista_num_neuronas_por_capas_oculta es una lista que en cada posicion tiene el numero de neuronas de esa capa oculta\n","\n","len(lista_num_neuronas_por_capas_oculta) es el num de capas\n","\n","devuelve la activacion para TODAS las CAPAS\n","\"\"\"\n"],"metadata":{"id":"L5lfgFwMgxaC"},"execution_count":null,"outputs":[]}]}